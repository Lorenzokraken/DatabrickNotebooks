{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a2f0d2",
   "metadata": {},
   "source": [
    "# üìö Lezione 1: ETL Base in Databricks\n",
    "\n",
    "## üìå Cos'√® ETL?\n",
    "\n",
    "ETL significa **Extract, Transform, Load** (Estrai, Trasforma, Carica):\n",
    "\n",
    "1. **Extract**: Leggi dati da fonti esterne (file, database, API, cloud storage)\n",
    "2. **Transform**: Pulisci, filtra, aggrega, unisci i dati\n",
    "3. **Load**: Salva i risultati in un sistema di destinazione (tabelle, data warehouse)\n",
    "\n",
    "### üéØ Perch√© ETL in Databricks?\n",
    "- **Scalabilit√†**: Spark processa grandi volumi distribuiti su cluster\n",
    "- **Integrazione**: Accesso a cloud storage (Azure, AWS, GCP)\n",
    "- **Delta Lake**: Formato ottimizzato con transazioni ACID\n",
    "- **Unified Analytics**: Stesso ambiente per data engineering, ML e analytics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc28674",
   "metadata": {},
   "source": [
    "## üîó 1. EXTRACT - Leggere Dati\n",
    "\n",
    "### Da File (CSV, JSON, Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed3955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leggere CSV\n",
    "df_csv = spark.read.csv(\"/path/to/data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Leggere JSON\n",
    "df_json = spark.read.json(\"/path/to/data.json\")\n",
    "\n",
    "# Leggere Parquet (formato ottimizzato)\n",
    "df_parquet = spark.read.parquet(\"/path/to/data.parquet\")\n",
    "\n",
    "# Esplorare\n",
    "df_csv.printSchema()\n",
    "df_csv.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f5af0c",
   "metadata": {},
   "source": [
    "### Da Tabelle Delta/Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57faa520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leggere da tabella esistente\n",
    "df_table = spark.read.table(\"catalog.schema.table_name\")\n",
    "\n",
    "# Oppure con SQL\n",
    "df_sql = spark.sql(\"SELECT * FROM catalog.schema.table_name\")\n",
    "\n",
    "df_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1499a990",
   "metadata": {},
   "source": [
    "### Da Azure Data Lake Storage (ADLS)\n",
    "\n",
    "Per accedere a cloud storage, si usa il mounting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4342fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione OAuth per ADLS Gen2\n",
    "configs = {\n",
    "    \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "    \"fs.azure.account.oauth2.client.id\": \"<client_id>\",\n",
    "    \"fs.azure.account.oauth2.client.secret\": \"<client_secret>\",\n",
    "    \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/<tenant_id>/oauth2/token\"\n",
    "}\n",
    "\n",
    "# Mount dello storage\n",
    "dbutils.fs.mount(\n",
    "    source = \"abfss://<container>@<storage_account>.dfs.core.windows.net/\",\n",
    "    mount_point = \"/mnt/datalake\",\n",
    "    extra_configs = configs\n",
    ")\n",
    "\n",
    "# Ora puoi leggere dal mount\n",
    "df = spark.read.csv(\"/mnt/datalake/data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df39e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ 2. TRANSFORM - Trasformare Dati\n",
    "\n",
    "### Pulizia Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ad7b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "# Rimuovere righe con valori null\n",
    "df_clean = df.filter(col('important_column').isNotNull())\n",
    "\n",
    "# Rimuovere duplicati\n",
    "df_clean = df.dropDuplicates(['id', 'email'])\n",
    "\n",
    "# Rinominare colonne\n",
    "df_clean = df.withColumnRenamed('old_name', 'new_name')\n",
    "\n",
    "# Rimuovere colonne non necessarie\n",
    "df_clean = df.drop('unwanted_col1', 'unwanted_col2')\n",
    "\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4ee2eb",
   "metadata": {},
   "source": [
    "### Creare Nuove Colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8836aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit, concat\n",
    "\n",
    "# Nuova colonna calcolata\n",
    "df_transformed = df.withColumn('total_price', col('quantity') * col('unit_price'))\n",
    "\n",
    "# Colonna con logica condizionale\n",
    "df_transformed = df.withColumn('price_category',\n",
    "    when(col('price') > 1000, 'Expensive')\n",
    "    .when(col('price') > 100, 'Medium')\n",
    "    .otherwise('Cheap')\n",
    ")\n",
    "\n",
    "# Colonna costante\n",
    "df_transformed = df.withColumn('country', lit('Italy'))\n",
    "\n",
    "# Concatenare colonne\n",
    "df_transformed = df.withColumn('full_name', \n",
    "    concat(col('first_name'), lit(' '), col('last_name'))\n",
    ")\n",
    "\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8e5ff",
   "metadata": {},
   "source": [
    "### Filtrare e Selezionare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97bc61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrare righe\n",
    "df_filtered = df.filter(col('age') > 18)\n",
    "df_filtered = df.filter((col('age') > 18) & (col('country') == 'Italy'))\n",
    "\n",
    "# Selezionare colonne specifiche\n",
    "df_selected = df.select('customer_id', 'name', 'total_amount')\n",
    "\n",
    "# Ordinare\n",
    "df_sorted = df.orderBy(col('date').desc())\n",
    "\n",
    "df_sorted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa2ce4",
   "metadata": {},
   "source": [
    "### Aggregazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a08e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, avg, count, max, min\n",
    "\n",
    "# Group by semplice\n",
    "df_grouped = df.groupBy('category').count()\n",
    "\n",
    "# Aggregazioni multiple\n",
    "df_agg = df.groupBy('category').agg(\n",
    "    count('*').alias('total_records'),\n",
    "    sum('amount').alias('total_amount'),\n",
    "    avg('amount').alias('avg_amount'),\n",
    "    max('amount').alias('max_amount'),\n",
    "    min('amount').alias('min_amount')\n",
    ")\n",
    "\n",
    "df_agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21293e30",
   "metadata": {},
   "source": [
    "### Join (Unire Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supponiamo di avere due DataFrame\n",
    "customers = spark.read.table(\"customers\")\n",
    "orders = spark.read.table(\"orders\")\n",
    "\n",
    "# Inner join (solo righe con match)\n",
    "df_joined = customers.join(orders, 'customer_id', 'inner')\n",
    "\n",
    "# Left join (tutte le righe di customers, anche senza ordini)\n",
    "df_left = customers.join(orders, 'customer_id', 'left')\n",
    "\n",
    "# Join con colonne diverse\n",
    "df_joined = customers.join(orders, customers.id == orders.cust_id, 'inner')\n",
    "\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ba2465",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ 3. LOAD - Salvare Risultati\n",
    "\n",
    "### Salvare come Tabella Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d34951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creare catalog e schema (una tantum)\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS my_catalog\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS my_catalog.my_schema\")\n",
    "\n",
    "# Salvare tabella\n",
    "df_result.write.mode('overwrite').saveAsTable('my_catalog.my_schema.customers_clean')\n",
    "\n",
    "# Mode options:\n",
    "# 'overwrite' - Sovrascrive tabella esistente\n",
    "# 'append' - Aggiunge righe a tabella esistente\n",
    "# 'error' - Fallisce se tabella esiste (default)\n",
    "# 'ignore' - Non fa nulla se tabella esiste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d40fb6",
   "metadata": {},
   "source": [
    "### Salvare come File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cfe253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvare come Parquet (raccomandato per performance)\n",
    "df_result.write.mode('overwrite').parquet('/mnt/datalake/output.parquet')\n",
    "\n",
    "# Salvare come CSV\n",
    "df_result.write.mode('overwrite').csv('/mnt/datalake/output.csv', header=True)\n",
    "\n",
    "# Salvare come Delta (formato ottimizzato)\n",
    "df_result.write.format('delta').mode('overwrite').save('/mnt/datalake/output_delta')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945a93ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Esempio Completo: Pipeline ETL\n",
    "\n",
    "### Scenario: Analisi Vendite E-commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fddd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, sum, avg, count\n",
    "\n",
    "# ========== EXTRACT ==========\n",
    "# Leggi dati grezzi\n",
    "orders_raw = spark.read.csv(\"/mnt/datalake/orders.csv\", header=True, inferSchema=True)\n",
    "customers_raw = spark.read.table(\"bronze.customers\")\n",
    "\n",
    "print(\"=== Dati Grezzi ===\")\n",
    "orders_raw.printSchema()\n",
    "orders_raw.show(5)\n",
    "print(f\"Totale ordini: {orders_raw.count()}\")\n",
    "\n",
    "# ========== TRANSFORM ==========\n",
    "# 1. Pulizia ordini\n",
    "orders_clean = (\n",
    "    orders_raw\n",
    "    .filter(col('amount').isNotNull())  # Rimuovi null\n",
    "    .filter(col('amount') > 0)          # Rimuovi valori negativi\n",
    "    .dropDuplicates(['order_id'])       # Rimuovi duplicati\n",
    ")\n",
    "\n",
    "# 2. Arricchimento: aggiungi categoria di spesa\n",
    "orders_enriched = orders_clean.withColumn('spending_level',\n",
    "    when(col('amount') > 1000, 'High')\n",
    "    .when(col('amount') > 100, 'Medium')\n",
    "    .otherwise('Low')\n",
    ")\n",
    "\n",
    "# 3. Join con customers\n",
    "orders_with_customers = orders_enriched.join(\n",
    "    customers_raw.select('customer_id', 'customer_name', 'country'),\n",
    "    'customer_id',\n",
    "    'inner'\n",
    ")\n",
    "\n",
    "print(\"=== Dati Trasformati ===\")\n",
    "orders_with_customers.show(5)\n",
    "\n",
    "# 4. Aggregazione: statistiche per paese\n",
    "country_stats = orders_with_customers.groupBy('country').agg(\n",
    "    count('*').alias('total_orders'),\n",
    "    sum('amount').alias('total_revenue'),\n",
    "    avg('amount').alias('avg_order_value')\n",
    ").orderBy(col('total_revenue').desc())\n",
    "\n",
    "print(\"=== Risultati Finali ===\")\n",
    "country_stats.show()\n",
    "\n",
    "# ========== LOAD ==========\n",
    "# Salva risultati in Silver/Gold layer\n",
    "orders_with_customers.write.mode('overwrite').saveAsTable('silver.orders_enriched')\n",
    "country_stats.write.mode('overwrite').saveAsTable('gold.country_revenue')\n",
    "\n",
    "print(\"‚úÖ Pipeline ETL completata!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb5f8ea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Architettura Medallion (Bronze-Silver-Gold)\n",
    "\n",
    "Best practice per organizzare i dati in Databricks:\n",
    "\n",
    "### üü§ Bronze Layer (Dati Grezzi)\n",
    "- Dati come arrivano dalla fonte\n",
    "- Nessuna trasformazione\n",
    "- Schema originale preservato\n",
    "```python\n",
    "df_raw.write.saveAsTable('bronze.orders_raw')\n",
    "```\n",
    "\n",
    "### ü•à Silver Layer (Dati Puliti)\n",
    "- Pulizia, deduplicazione, validazione\n",
    "- Standardizzazione tipi di dati\n",
    "- Join base\n",
    "```python\n",
    "df_clean.write.saveAsTable('silver.orders_clean')\n",
    "```\n",
    "\n",
    "### ü•á Gold Layer (Dati Aggregati)\n",
    "- Dati pronti per analytics/BI\n",
    "- Aggregazioni, metriche business\n",
    "- Ottimizzati per query\n",
    "```python\n",
    "df_aggregated.write.saveAsTable('gold.daily_sales')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10e6ce8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° Best Practices ETL\n",
    "\n",
    "### 1. **Usa sempre `inferSchema=True` con cautela**\n",
    "```python\n",
    "# ‚ö†Ô∏è Va bene per esplorare, ma lento su grandi dataset\n",
    "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# ‚úÖ Meglio: definisci schema esplicito per performance\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "df = spark.read.csv(\"data.csv\", header=True, schema=schema)\n",
    "```\n",
    "\n",
    "### 2. **Partiziona i dati grandi**\n",
    "```python\n",
    "# Salva partizionato per query pi√π veloci\n",
    "df.write.partitionBy('year', 'month').saveAsTable('my_table')\n",
    "```\n",
    "\n",
    "### 3. **Cache per riutilizzo**\n",
    "```python\n",
    "# Se usi lo stesso DataFrame molte volte\n",
    "df_clean = df.filter(col('valid') == True).cache()\n",
    "df_clean.count()  # Materializza cache\n",
    "```\n",
    "\n",
    "### 4. **Monitora sempre le trasformazioni**\n",
    "```python\n",
    "# Verifica conteggi a ogni step\n",
    "print(f\"Righe iniziali: {df_raw.count()}\")\n",
    "print(f\"Dopo pulizia: {df_clean.count()}\")\n",
    "print(f\"Dopo join: {df_joined.count()}\")\n",
    "```\n",
    "\n",
    "### 5. **Delta Lake per produzione**\n",
    "```python\n",
    "# Sempre usa Delta per ACID transactions e time travel\n",
    "df.write.format('delta').mode('overwrite').saveAsTable('my_table')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e76a2e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Prossimi Passi\n",
    "\n",
    "Ora che conosci ETL base:\n",
    "1. **Pratica**: Vedi [PRATICA.ipynb](../PRATICA.ipynb) per esercizi con Delta Live Tables\n",
    "2. **Architettura**: Vedi [LEZIONE 2 APPUNTI.IPYNB](LEZIONE 2 APPUNTI.IPYNB) per Lakehouse Architecture\n",
    "3. **Avanzato**: Vedi [LEZIONE 3 APPUNTI.IPYNB](LEZIONE 3 APPUNTI.IPYNB) per Unity Catalog e integrazione ADLS\n",
    "\n",
    "### üîë Ricorda:\n",
    "- **Extract**: `spark.read.*`\n",
    "- **Transform**: `.filter()`, `.withColumn()`, `.groupBy()`, `.join()`\n",
    "- **Load**: `.write.saveAsTable()` o `.write.parquet()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3903dfc3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea836400",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
