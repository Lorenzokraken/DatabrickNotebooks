{
 "cells": [
  {
   "cell_type": "code",
   "id": "3rj4mkoijsm",
   "source": "# ‚ùå MAI usare pandas - usa sempre Spark!\n# ‚ùå SBAGLIATO: df.toPandas().groupby('col').sum()\n# ‚úÖ CORRETTO: df.groupBy('col').sum()\n\n# Partizionamento per performance\ndf.write.partitionBy(\"year\", \"month\").parquet(\"path/to/data\")\n\n# Broadcast join per tabelle piccole (<200MB)\nfrom pyspark.sql.functions import broadcast\nlarge_df.join(broadcast(small_df), \"key\").show()\n\n# Repartition vs Coalesce\ndf.repartition(200)  # Redistribuisce uniformemente (costoso)\ndf.coalesce(50)      # Riduce partizioni senza shuffle (veloce)\n\n# Cache su storage tiers\ndf.cache()                           # MEMORY_AND_DISK\ndf.persist(StorageLevel.DISK_ONLY)   # Solo disco\ndf.persist(StorageLevel.MEMORY_ONLY) # Solo memoria\n\n# Checkpoint per interrompere lineage lunghi\ndf.checkpoint()  # Salva su disco e tronca lineage\n\n# Configurazioni cluster per performance\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fu808xmjy0c",
   "source": "## üöÄ Performance Tips (Solo Spark!)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "74d0z0yvls",
   "source": "# Slowly Changing Dimension (SCD Type 2)\nfrom pyspark.sql.functions import current_timestamp, when, col\n\ndef scd_type2_merge(target_table, source_df, key_cols, compare_cols):\n    # Identifica record cambiati\n    changed_records = source_df.join(\n        target_table.filter(col(\"is_current\") == True),\n        key_cols\n    ).where(\n        # Confronta colonne per trovare cambiamenti\n        reduce(lambda x, y: x | y, [col(f\"source.{c}\") != col(f\"target.{c}\") for c in compare_cols])\n    )\n    \n    # Chiudi record vecchi\n    target_table.alias(\"target\").merge(\n        changed_records.alias(\"source\"),\n        \" AND \".join([f\"target.{k} = source.{k}\" for k in key_cols])\n    ).whenMatchedUpdate(set={\n        \"is_current\": lit(False),\n        \"end_date\": current_timestamp()\n    }).execute()\n    \n    # Inserisci nuovi record\n    new_records = changed_records.select(\"source.*\").withColumn(\"start_date\", current_timestamp()).withColumn(\"is_current\", lit(True))\n    target_table.alias(\"target\").merge(\n        new_records.alias(\"source\"),\n        \"1=2\"  # Never match, always insert\n    ).whenNotMatchedInsertAll().execute()\n\n# Window Functions per ranking\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, rank, dense_rank\n\nwindow_spec = Window.partitionBy(\"customer_id\").orderBy(desc(\"order_date\"))\ndf.withColumn(\"row_num\", row_number().over(window_spec)).show()\n\n# Deduplicazione\ndf.dropDuplicates([\"customer_id\", \"email\"]).show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7knsewoyt2s",
   "source": "## üîÑ ETL Patterns Avanzati",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "i66z28dwdj7",
   "source": "# Mount Azure Data Lake Storage (ADLS)\nconfigs = {\n    \"fs.azure.account.auth.type\": \"OAuth\",\n    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n    \"fs.azure.account.oauth2.client.id\": \"your_client_id\",\n    \"fs.azure.account.oauth2.client.secret\": \"your_client_secret\",\n    \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/your_tenant_id/oauth2/token\"\n}\n\ndbutils.fs.mount(\n    source = \"abfss://container@storageaccount.dfs.core.windows.net/\",\n    mount_point = \"/mnt/datalake\",\n    extra_configs = configs\n)\n\n# Mount AWS S3\ndbutils.fs.mount(\n    source = \"s3a://your-bucket-name\",\n    mount_point = \"/mnt/s3bucket\",\n    extra_configs = {\n        \"fs.s3a.access.key\": \"your_access_key\",\n        \"fs.s3a.secret.key\": \"your_secret_key\"\n    }\n)\n\n# Vedere mount points\ndbutils.fs.mounts()\n\n# Unmount\ndbutils.fs.unmount(\"/mnt/datalake\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "zgvrubt1tmf",
   "source": "## üîó Storage Mounting (ADLS/S3)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "588lbb8zi7s",
   "source": "# Salva come Delta Table\ndf.write.format(\"delta\").saveAsTable(\"my_database.delta_table\")\n\n# Merge (UPSERT) - aggiorna se esiste, inserisce se nuovo\nfrom delta.tables import DeltaTable\n\ndelta_table = DeltaTable.forName(spark, \"my_database.customers\")\n\ndelta_table.alias(\"target\").merge(\n    new_data.alias(\"source\"),\n    \"target.customer_id = source.customer_id\"\n).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n\n# Time Travel - versioni precedenti\nspark.sql(\"SELECT * FROM my_database.customers VERSION AS OF 1\").show()\nspark.sql(\"SELECT * FROM my_database.customers TIMESTAMP AS OF '2024-01-01'\").show()\n\n# Ottimizzare tabella (compatta piccoli file)\nspark.sql(\"OPTIMIZE my_database.customers\")\n\n# Z-Order per performance query\nspark.sql(\"OPTIMIZE my_database.customers ZORDER BY (customer_id)\")\n\n# Vacuum - rimuove file vecchi (default 7 giorni)\nspark.sql(\"VACUUM my_database.customers\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "hw3k5bdiwss",
   "source": "## üî∫ Delta Lake Operations",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "fmpkoknzcx",
   "source": "# Creare database\nspark.sql(\"CREATE DATABASE IF NOT EXISTS my_database\")\n\n# Usare database\nspark.sql(\"USE my_database\")\n\n# Creare tabella da DataFrame\ndf.write.saveAsTable(\"my_database.customers\")\n\n# Creare tabella vuota con schema\nspark.sql(\"\"\"\n    CREATE TABLE IF NOT EXISTS my_database.orders (\n        order_id INT,\n        customer_id INT,\n        order_date DATE,\n        amount DECIMAL(10,2)\n    )\n\"\"\")\n\n# Vedere tabelle disponibili\nspark.sql(\"SHOW TABLES IN my_database\").show()\n\n# Descrivere tabella\nspark.sql(\"DESCRIBE my_database.customers\").show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dyvnhtp1vas",
   "source": "## üóÑÔ∏è Unity Catalog - Database e Tabelle",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h9661s06je",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Leggi i dati\n",
    "df = spark.read.csv(\"path/to/data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2. Esplora\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "print(f\"Righe totali: {df.count()}\")\n",
    "\n",
    "# 3. Pulisci (se necessario)\n",
    "df_clean = df.filter(df.column.isNotNull())\n",
    "\n",
    "# 4. Trasforma\n",
    "df_transformed = df_clean.withColumn(\"new_col\", col(\"old_col\") * 2)\n",
    "\n",
    "# 5. Analizza\n",
    "result = df_transformed.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    avg(\"value\").alias(\"avg_value\")\n",
    ")\n",
    "\n",
    "# 6. Mostra risultato\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fmq7rb47w1w",
   "metadata": {},
   "source": [
    "## üéØ Pattern Comuni per Esercizi\n",
    "\n",
    "### üìù Template Base Esercizio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xbrj1o7d0ks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registra DataFrame come vista temporanea\n",
    "df.createOrReplaceTempView(\"my_data\")\n",
    "\n",
    "# Ora puoi usare SQL\n",
    "result = spark.sql(\"SELECT * FROM my_data WHERE age > 25\")\n",
    "result.show()\n",
    "\n",
    "# Query complesse\n",
    "spark.sql(\"\"\"\n",
    "    SELECT category, \n",
    "           COUNT(*) as total_records,\n",
    "           AVG(price) as avg_price\n",
    "    FROM my_data \n",
    "    GROUP BY category\n",
    "    ORDER BY avg_price DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wnm3pffdpz",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è SQL Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o31uf8e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva come tabella\n",
    "df.write.saveAsTable(\"my_database.my_table\")\n",
    "\n",
    "# Salva file Parquet\n",
    "df.write.parquet(\"path/to/output.parquet\")\n",
    "\n",
    "# Salva CSV\n",
    "df.write.csv(\"path/to/output.csv\", header=True)\n",
    "\n",
    "# Modalit√† di scrittura\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"table\")   # Sovrascrive\n",
    "df.write.mode(\"append\").saveAsTable(\"table\")      # Aggiunge\n",
    "\n",
    "# Cache per performance\n",
    "df.cache()                    # Mantiene in memoria\n",
    "df.persist()                  # Pi√π controllo storage\n",
    "df.unpersist()               # Rimuove dalla cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eonv318ivo9",
   "metadata": {},
   "source": [
    "## üíæ Salvataggio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uzorbl7e9ws",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join tipi\n",
    "df1.join(df2, \"common_column\").show()                    # Inner join\n",
    "df1.join(df2, \"common_column\", \"inner\").show()           # Inner join esplicito\n",
    "df1.join(df2, \"common_column\", \"left\").show()            # Left join\n",
    "df1.join(df2, \"common_column\", \"right\").show()           # Right join\n",
    "df1.join(df2, \"common_column\", \"outer\").show()           # Full outer join\n",
    "\n",
    "# Join con colonne diverse\n",
    "df1.join(df2, df1.id == df2.user_id).show()\n",
    "\n",
    "# Join multipli\n",
    "df1.join(df2, \"id\").join(df3, \"category_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v9f5mvae1se",
   "metadata": {},
   "source": [
    "## üîó Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0oc2iww15kj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group By base\n",
    "df.groupBy(\"category\").count().show()\n",
    "df.groupBy(\"category\").sum(\"amount\").show()\n",
    "df.groupBy(\"category\").avg(\"price\").show()\n",
    "df.groupBy(\"category\").max(\"date\").show()\n",
    "df.groupBy(\"category\").min(\"date\").show()\n",
    "\n",
    "# Aggregazioni multiple\n",
    "df.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"total_records\"),\n",
    "    sum(\"amount\").alias(\"total_amount\"),\n",
    "    avg(\"price\").alias(\"avg_price\")\n",
    ").show()\n",
    "\n",
    "# Senza groupBy\n",
    "df.agg(count(\"*\"), sum(\"amount\"), avg(\"price\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0t26zp5xpw4",
   "metadata": {},
   "source": [
    "## üìä Aggregazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68cgfehmc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Aggiungere/modificare colonne\n",
    "df.withColumn(\"new_col\", lit(\"value\")).show()          # Colonna costante\n",
    "df.withColumn(\"age_plus_10\", col(\"age\") + 10).show()   # Calcolo\n",
    "df.withColumnRenamed(\"old_name\", \"new_name\").show()    # Rinomina\n",
    "\n",
    "# Ordinamento\n",
    "df.orderBy(\"age\").show()                # Crescente\n",
    "df.orderBy(desc(\"age\")).show()          # Decrescente\n",
    "\n",
    "# Drop colonne\n",
    "df.drop(\"unwanted_col\").show()\n",
    "\n",
    "# Cast tipi\n",
    "df.withColumn(\"age_string\", col(\"age\").cast(\"string\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6pahdzmii9s",
   "metadata": {},
   "source": [
    "## üîÑ Trasformazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "njz5z7xlzj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selezione colonne\n",
    "df.select(\"col1\", \"col2\").show()\n",
    "df.select(\"*\").show()                    # Tutte le colonne\n",
    "\n",
    "# Filtri\n",
    "df.filter(df.age > 18).show()           # Condizione semplice\n",
    "df.filter((df.age > 18) & (df.city == \"Rome\")).show()  # AND\n",
    "df.filter((df.age < 18) | (df.age > 65)).show()        # OR\n",
    "\n",
    "# Where (identico a filter)\n",
    "df.where(df.status == \"active\").show()\n",
    "\n",
    "# Limit\n",
    "df.limit(100).show()                    # Prime 100 righe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hgem8rvwtw",
   "metadata": {},
   "source": [
    "## üéØ Selezione e Filtri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87svbg408k3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema e info base\n",
    "df.printSchema()           # Struttura colonne\n",
    "df.show()                  # Prime 20 righe\n",
    "df.show(5)                 # Prime 5 righe\n",
    "df.count()                 # Numero righe\n",
    "df.columns                 # Lista colonne\n",
    "\n",
    "# Statistiche descrittive\n",
    "df.describe().show()       # Count, mean, stddev, min, max\n",
    "df.summary().show()        # Pi√π dettagliate\n",
    "\n",
    "# Info specifiche colonne\n",
    "df.select(\"column_name\").distinct().show()  # Valori unici\n",
    "df.groupBy(\"column\").count().show()         # Conteggi per gruppo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xxatyre2vnp",
   "metadata": {},
   "source": [
    "## üîç Esplorazione Dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ghau7af2kzg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lettura file CSV\n",
    "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Lettura JSON\n",
    "df = spark.read.json(\"path/to/file.json\")\n",
    "\n",
    "# Lettura Parquet\n",
    "df = spark.read.parquet(\"path/to/file.parquet\")\n",
    "\n",
    "# Da database\n",
    "df = spark.read.table(\"database.table_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zeau3tx39kh",
   "metadata": {},
   "source": [
    "# üîß Databricks - Funzioni Essenziali per Esercizi\n",
    "\n",
    "## üìä Lettura Dati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xfkjiedaab",
   "metadata": {},
   "source": [
    "# üìä Databricks: Da Noob a Worker\n",
    "\n",
    "## üéØ Roadmap Completa per Diventare Esperto Databricks\n",
    "\n",
    "### üìã Indice:\n",
    "1. **Setup & Basics** - Ambiente e concetti base\n",
    "2. **Apache Spark Fundamentals** - Core engine \n",
    "3. **Data Engineering** - ETL e pipeline\n",
    "4. **Machine Learning** - MLflow e AutoML\n",
    "5. **Advanced Topics** - Performance tuning\n",
    "6. **Best Practices** - Patterns produttivi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xfkjiedaab",
   "metadata": {},
   "source": [
    "# üìä Databricks: Da Noob a Worker\n",
    "\n",
    "## üéØ Roadmap Completa per Diventare Esperto Databricks\n",
    "\n",
    "### üìã Indice:\n",
    "1. **Setup & Basics** - Ambiente e concetti base\n",
    "2. **Apache Spark Fundamentals** - Core engine \n",
    "3. **Data Engineering** - ETL e pipeline\n",
    "4. **Machine Learning** - MLflow e AutoML\n",
    "5. **Advanced Topics** - Performance tuning\n",
    "6. **Best Practices** - Patterns produttivi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e74f68",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark.read.json(\"data/retail-data/all/\").printSchema()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}