{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5582288c",
   "metadata": {},
   "source": [
    "## ðŸ“Š Pandas vs Spark: Equivalenze Base\n",
    "\n",
    "| Operazione | Pandas | Spark |\n",
    "|-----------|--------|-------|\n",
    "| **Leggere CSV** | `pd.read_csv(\"file.csv\")` | `spark.read.csv(\"file.csv\", header=True, inferSchema=True)` |\n",
    "| **Prime righe** | `df.head()` | `df.show()` |\n",
    "| **Info struttura** | `df.info()` | `df.printSchema()` |\n",
    "| **Contare righe** | `len(df)` | `df.count()` |\n",
    "| **Statistiche** | `df.describe()` | `df.describe().show()` |\n",
    "| **Filtrare** | `df[df['age'] > 18]` | `df.filter(df.age > 18)` |\n",
    "| **Selezionare colonne** | `df[['col1', 'col2']]` | `df.select('col1', 'col2')` |\n",
    "| **Nuova colonna** | `df['new'] = df['old'] * 2` | `df.withColumn('new', col('old') * 2)` |\n",
    "| **Ordinare** | `df.sort_values('age')` | `df.orderBy('age')` |\n",
    "| **Group by** | `df.groupby('category').sum()` | `df.groupBy('category').sum()` |\n",
    "| **Join** | `df1.merge(df2, on='id')` | `df1.join(df2, 'id')` |\n",
    "\n",
    "### âš ï¸ DIFFERENZA CHIAVE: Spark Ã¨ immutabile!\n",
    "\n",
    "```python\n",
    "# âŒ PANDAS: Modifica in-place\n",
    "df['new_col'] = df['old_col'] * 2  # Modifica df\n",
    "\n",
    "# âœ… SPARK: Crea nuovo DataFrame\n",
    "df_new = df.withColumn('new_col', col('old_col') * 2)  # df rimane uguale!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d24e1a",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Esempio Pratico 1: Caricamento e Esplorazione\n",
    "\n",
    "### Con Pandas (su dati piccoli):\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"customers.csv\")\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(df['age'].mean())\n",
    "```\n",
    "\n",
    "### Con Spark (su dati grandi):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d1c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark legge da file o tabelle\n",
    "df = spark.read.csv(\"/path/to/customers.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Esplora struttura\n",
    "df.printSchema()  # Come df.info() in Pandas\n",
    "\n",
    "# Prime righe\n",
    "df.show(5)  # Come df.head() in Pandas\n",
    "\n",
    "# Conta righe\n",
    "print(f\"Totale righe: {df.count()}\")  # Come len(df)\n",
    "\n",
    "# Statistiche\n",
    "df.describe().show()  # Come df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f24daa",
   "metadata": {},
   "source": [
    "## ðŸ”„ Esempio Pratico 2: Trasformazioni\n",
    "\n",
    "### Con Pandas:\n",
    "```python\n",
    "# Filtro\n",
    "df_filtered = df[df['age'] > 25]\n",
    "\n",
    "# Nuova colonna\n",
    "df['age_group'] = df['age'].apply(lambda x: 'Senior' if x > 60 else 'Adult')\n",
    "\n",
    "# Seleziona colonne\n",
    "df_subset = df[['name', 'age', 'age_group']]\n",
    "```\n",
    "\n",
    "### Con Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Filtro (come Pandas)\n",
    "df_filtered = df.filter(col('age') > 25)\n",
    "\n",
    "# Nuova colonna con logica condizionale\n",
    "df_transformed = df.withColumn('age_group', \n",
    "    when(col('age') > 60, 'Senior')\n",
    "    .otherwise('Adult')\n",
    ")\n",
    "\n",
    "# Seleziona colonne\n",
    "df_subset = df_transformed.select('name', 'age', 'age_group')\n",
    "\n",
    "# Mostra risultato\n",
    "df_subset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a30ff3a",
   "metadata": {},
   "source": [
    "## ðŸ“Š Esempio Pratico 3: Aggregazioni\n",
    "\n",
    "### Con Pandas:\n",
    "```python\n",
    "# Group by base\n",
    "df.groupby('category')['amount'].sum()\n",
    "\n",
    "# Aggregazioni multiple\n",
    "df.groupby('category').agg({\n",
    "    'amount': ['sum', 'mean'],\n",
    "    'customer_id': 'count'\n",
    "})\n",
    "```\n",
    "\n",
    "### Con Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a604f986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, avg, count\n",
    "\n",
    "# Group by base\n",
    "df.groupBy('category').sum('amount').show()\n",
    "\n",
    "# Aggregazioni multiple (come Pandas .agg())\n",
    "df.groupBy('category').agg(\n",
    "    sum('amount').alias('total_amount'),\n",
    "    avg('amount').alias('avg_amount'),\n",
    "    count('customer_id').alias('num_customers')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881255b6",
   "metadata": {},
   "source": [
    "## ðŸ”— Esempio Pratico 4: Join (come merge in Pandas)\n",
    "\n",
    "### Con Pandas:\n",
    "```python\n",
    "# Inner join\n",
    "result = df1.merge(df2, on='customer_id', how='inner')\n",
    "\n",
    "# Left join\n",
    "result = df1.merge(df2, on='customer_id', how='left')\n",
    "```\n",
    "\n",
    "### Con Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e6d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supponiamo di avere due tabelle: customers e orders\n",
    "\n",
    "customers = spark.read.table(\"customers\")\n",
    "orders = spark.read.table(\"orders\")\n",
    "\n",
    "# Inner join (default)\n",
    "result = customers.join(orders, 'customer_id', 'inner')\n",
    "\n",
    "# Left join\n",
    "result = customers.join(orders, 'customer_id', 'left')\n",
    "\n",
    "# Join con colonne diverse\n",
    "result = customers.join(orders, customers.id == orders.customer_id, 'inner')\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38466628",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Esempio Pratico 5: Salvare Risultati\n",
    "\n",
    "### Con Pandas:\n",
    "```python\n",
    "# Salva CSV\n",
    "df.to_csv('output.csv', index=False)\n",
    "\n",
    "# Salva Parquet\n",
    "df.to_parquet('output.parquet')\n",
    "```\n",
    "\n",
    "### Con Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1831170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva come tabella (modo piÃ¹ comune in Databricks)\n",
    "df.write.mode('overwrite').saveAsTable('my_database.results')\n",
    "\n",
    "# Salva come file Parquet\n",
    "df.write.mode('overwrite').parquet('/path/to/output.parquet')\n",
    "\n",
    "# Salva CSV\n",
    "df.write.mode('overwrite').csv('/path/to/output.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4de38e",
   "metadata": {},
   "source": [
    "## ðŸ—„ï¸ Databricks Specifico: Unity Catalog\n",
    "\n",
    "In Databricks, i dati sono organizzati in modo gerarchico:\n",
    "\n",
    "```\n",
    "Catalog (come un database server)\n",
    "  â””â”€ Schema (come un database)\n",
    "      â””â”€ Tabelle\n",
    "```\n",
    "\n",
    "### Creare struttura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d572ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea catalog\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS my_catalog\")\n",
    "\n",
    "# Crea schema (database)\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS my_catalog.my_schema\")\n",
    "\n",
    "# Usa lo schema\n",
    "spark.sql(\"USE my_catalog.my_schema\")\n",
    "\n",
    "# Salva tabella\n",
    "df.write.saveAsTable(\"my_catalog.my_schema.my_table\")\n",
    "\n",
    "# Leggi tabella\n",
    "df = spark.read.table(\"my_catalog.my_schema.my_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7440c400",
   "metadata": {},
   "source": [
    "## ðŸš€ Esempio Completo: Pipeline ETL Base\n",
    "\n",
    "### Template standard per ogni esercizio Databricks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b56b6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, sum, avg, count\n",
    "\n",
    "# ========== 1. ESTRAZIONE (Extract) ==========\n",
    "# Leggi dati da file o tabella\n",
    "df_raw = spark.read.csv(\"/path/to/data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# ========== 2. ESPLORAZIONE ==========\n",
    "print(\"Schema:\")\n",
    "df_raw.printSchema()\n",
    "\n",
    "print(\"\\nPrime 5 righe:\")\n",
    "df_raw.show(5)\n",
    "\n",
    "print(f\"\\nTotale righe: {df_raw.count()}\")\n",
    "\n",
    "# ========== 3. TRASFORMAZIONE (Transform) ==========\n",
    "# Pulizia\n",
    "df_clean = df_raw.filter(col('amount').isNotNull())\n",
    "\n",
    "# Nuove colonne\n",
    "df_transformed = df_clean.withColumn('amount_category',\n",
    "    when(col('amount') > 1000, 'High')\n",
    "    .when(col('amount') > 100, 'Medium')\n",
    "    .otherwise('Low')\n",
    ")\n",
    "\n",
    "# ========== 4. AGGREGAZIONE ==========\n",
    "df_result = df_transformed.groupBy('category').agg(\n",
    "    count('*').alias('total_records'),\n",
    "    sum('amount').alias('total_amount'),\n",
    "    avg('amount').alias('avg_amount')\n",
    ")\n",
    "\n",
    "# ========== 5. CARICAMENTO (Load) ==========\n",
    "# Mostra risultato\n",
    "df_result.show()\n",
    "\n",
    "# Salva tabella\n",
    "df_result.write.mode('overwrite').saveAsTable('my_catalog.my_schema.results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a5cab",
   "metadata": {},
   "source": [
    "## âš ï¸ Errori Comuni da Evitare\n",
    "\n",
    "```python\n",
    "# âŒ SBAGLIATO: Porta tutto in memoria su una sola macchina\n",
    "df_pandas = df.toPandas()\n",
    "df_pandas.groupby('col').sum()\n",
    "\n",
    "# âœ… CORRETTO: Usa Spark distribuito\n",
    "df.groupBy('col').sum().show()\n",
    "```\n",
    "\n",
    "### 2. âŒ NON iterare riga per riga!\n",
    "```python\n",
    "# âŒ SBAGLIATO: Lentissimo\n",
    "for row in df.collect():\n",
    "    process_row(row)\n",
    "\n",
    "# âœ… CORRETTO: Usa trasformazioni vettoriali\n",
    "df.withColumn('new_col', col('old_col') * 2)\n",
    "```\n",
    "\n",
    "### 3. âš ï¸ `.show()` vs `.collect()` vs `.display()`\n",
    "```python\n",
    "df.show()      # Mostra prime 20 righe (sicuro)\n",
    "df.display()   # Databricks-only: visualizzazione interattiva (sicuro)\n",
    "df.collect()   # Porta TUTTO in memoria (pericoloso con big data!)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9ebfcd",
   "metadata": {},
   "source": [
    "## ðŸ”¥ Best Practices\n",
    "\n",
    "### 1. **Lazy Evaluation**: Spark costruisce un piano, esegue solo quando necessario\n",
    "```python\n",
    "# Questi NON vengono eseguiti subito\n",
    "df1 = df.filter(col('age') > 18)\n",
    "df2 = df1.withColumn('new_col', col('old_col') * 2)\n",
    "df3 = df2.groupBy('category').count()\n",
    "\n",
    "# Esegue SOLO qui (Action)\n",
    "df3.show()  # Ora Spark esegue tutto insieme, ottimizzato\n",
    "```\n",
    "\n",
    "### 2. **Cache per riutilizzare dati**\n",
    "```python\n",
    "# Se usi lo stesso DataFrame molte volte\n",
    "df_filtered = df.filter(col('age') > 18).cache()\n",
    "\n",
    "df_filtered.count()  # Esegue e salva in memoria\n",
    "df_filtered.show()   # Usa cache, non ricalcola\n",
    "```\n",
    "\n",
    "### 3. **Usa SQL quando Ã¨ piÃ¹ chiaro**\n",
    "```python\n",
    "# Registra come vista temporanea\n",
    "df.createOrReplaceTempView('my_data')\n",
    "\n",
    "# Usa SQL (utile se conosci bene SQL)\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT category, COUNT(*) as count, AVG(amount) as avg_amount\n",
    "    FROM my_data\n",
    "    WHERE amount > 100\n",
    "    GROUP BY category\n",
    "    ORDER BY avg_amount DESC\n",
    "\"\"\")\n",
    "result.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409f7086",
   "metadata": {},
   "source": [
    "## ðŸ“š Prossimi Passi\n",
    "\n",
    "Ora che conosci le basi, esplora:\n",
    "\n",
    "1. **[databricks_cheatsheet.ipynb](databricks_cheatsheet.ipynb)** - Comandi ordinati per complessitÃ \n",
    "2. **[PRATICA.ipynb](../PRATICA.ipynb)** - Esercizi pratici con Delta Live Tables\n",
    "3. **[LEZIONI/](../LEZIONI/)** - Approfondimenti su architettura e concetti\n",
    "\n",
    "### ðŸŽ¯ Cosa fare per imparare:\n",
    "1. Prendi un tuo progetto Pandas esistente\n",
    "2. Prova a riscriverlo in Spark usando questo notebook come riferimento\n",
    "3. Sperimenta con dataset piÃ¹ grandi (dove Pandas non ce la fa)\n",
    "\n",
    "### ðŸ†˜ Risorse utili:\n",
    "- [Databricks Community Edition](https://community.cloud.databricks.com/) - Gratis per imparare\n",
    "- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [Pandas to Spark Cheat Sheet](https://sparkbyexamples.com/pandas/pandas-vs-pyspark/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
