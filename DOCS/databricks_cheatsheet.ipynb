{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Per chi viene da Pandas/ML\n",
    "\n",
    "Se conosci **Pandas**, Spark √® simile ma con differenze chiave:\n",
    "- **Distribuito**: i dati sono su pi√π macchine, non su una sola\n",
    "- **Lazy**: Spark non esegue subito, pianifica ed esegue quando serve\n",
    "- **Immutabile**: ogni trasformazione crea un nuovo DataFrame\n",
    "\n",
    "### üîë Equivalenze rapide:\n",
    "```python\n",
    "# Pandas ‚Üí Spark\n",
    "df.head()               ‚Üí df.show()\n",
    "df.info()               ‚Üí df.printSchema()\n",
    "len(df)                 ‚Üí df.count()\n",
    "df[df['age'] > 18]      ‚Üí df.filter(col('age') > 18)\n",
    "df[['col1', 'col2']]    ‚Üí df.select('col1', 'col2')\n",
    "df.groupby('x').sum()   ‚Üí df.groupBy('x').sum()\n",
    "```\n",
    "\n",
    "### ‚õî MAI fare:\n",
    "```python\n",
    "# ‚ùå NON convertire a Pandas per operazioni su grandi dataset\n",
    "df.toPandas().groupby('col').sum()  # SBAGLIATO: porta tutto in memoria!\n",
    "\n",
    "# ‚úÖ USA Spark distribuito\n",
    "df.groupBy('col').sum()  # CORRETTO: resta distribuito\n",
    "```\n",
    "\n",
    "üìò **Guida completa**: Vedi [PANDAS_TO_SPARK.ipynb](PANDAS_TO_SPARK.ipynb) per capire le differenze in dettaglio.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîß Databricks Cheatsheet - In ordine di complessit√†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 1. Lettura Dati Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lettura file CSV\n",
    "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Lettura JSON\n",
    "df = spark.read.json(\"path/to/file.json\")\n",
    "\n",
    "# Lettura Parquet\n",
    "df = spark.read.parquet(\"path/to/file.parquet\")\n",
    "\n",
    "# Da tabella\n",
    "df = spark.read.table(\"database.table_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 2. Esplorazione Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info struttura\n",
    "df.printSchema()    # Schema colonne\n",
    "df.show(5)          # Prime 5 righe\n",
    "df.count()          # Totale righe\n",
    "df.columns          # Lista colonne\n",
    "\n",
    "# Statistiche base\n",
    "df.describe().show()\n",
    "df.select(\"column_name\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ 3. Selezione e Filtri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selezione colonne\n",
    "df.select(\"col1\", \"col2\").show()\n",
    "\n",
    "# Filtri base\n",
    "df.filter(df.age > 18).show()\n",
    "df.filter((df.age > 18) & (df.city == \"Rome\")).show()\n",
    "df.where(df.status == \"active\").show()\n",
    "\n",
    "# Limit\n",
    "df.limit(100).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ 4. Trasformazioni Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Nuove colonne\n",
    "df.withColumn(\"age_plus_10\", col(\"age\") + 10).show()\n",
    "df.withColumn(\"constant\", lit(\"value\")).show()\n",
    "\n",
    "# Rinominare\n",
    "df.withColumnRenamed(\"old_name\", \"new_name\").show()\n",
    "\n",
    "# Ordinamento\n",
    "df.orderBy(\"age\").show()\n",
    "df.orderBy(desc(\"age\")).show()\n",
    "\n",
    "# Drop colonne\n",
    "df.drop(\"unwanted_col\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 5. Aggregazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group By base\n",
    "df.groupBy(\"category\").count().show()\n",
    "df.groupBy(\"category\").sum(\"amount\").show()\n",
    "df.groupBy(\"category\").avg(\"price\").show()\n",
    "\n",
    "# Aggregazioni multiple\n",
    "df.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"total_records\"),\n",
    "    sum(\"amount\").alias(\"total_amount\"),\n",
    "    avg(\"price\").alias(\"avg_price\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó 6. Join Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join tipi\n",
    "df1.join(df2, \"common_column\").show()           # Inner\n",
    "df1.join(df2, \"common_column\", \"left\").show()   # Left\n",
    "df1.join(df2, \"common_column\", \"right\").show()  # Right\n",
    "df1.join(df2, \"common_column\", \"outer\").show()  # Full outer\n",
    "\n",
    "# Join con colonne diverse\n",
    "df1.join(df2, df1.id == df2.user_id).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ 7. Salvataggio Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva tabella\n",
    "df.write.saveAsTable(\"my_database.my_table\")\n",
    "\n",
    "# Salva file\n",
    "df.write.parquet(\"path/to/output.parquet\")\n",
    "df.write.csv(\"path/to/output.csv\", header=True)\n",
    "\n",
    "# Modalit√† scrittura\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"table\")\n",
    "df.write.mode(\"append\").saveAsTable(\"table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è 8. SQL Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registra come vista\n",
    "df.createOrReplaceTempView(\"my_data\")\n",
    "\n",
    "# Usa SQL\n",
    "result = spark.sql(\"SELECT * FROM my_data WHERE age > 25\")\n",
    "\n",
    "# Query complesse\n",
    "spark.sql(\"\"\"\n",
    "    SELECT category, COUNT(*) as count, AVG(price) as avg_price\n",
    "    FROM my_data \n",
    "    GROUP BY category\n",
    "    ORDER BY avg_price DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è 9. Unity Catalog - Database/Tabelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creare database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS my_database\")\n",
    "spark.sql(\"USE my_database\")\n",
    "\n",
    "# Creare tabella da DataFrame\n",
    "df.write.saveAsTable(\"my_database.customers\")\n",
    "\n",
    "# Creare tabella vuota\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS my_database.orders (\n",
    "        order_id INT,\n",
    "        customer_id INT,\n",
    "        order_date DATE,\n",
    "        amount DECIMAL(10,2)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Info tabelle\n",
    "spark.sql(\"SHOW TABLES IN my_database\").show()\n",
    "spark.sql(\"DESCRIBE my_database.customers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî∫ 10. Delta Lake Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Salva come Delta\n",
    "df.write.format(\"delta\").saveAsTable(\"my_database.delta_table\")\n",
    "\n",
    "# Merge (UPSERT)\n",
    "delta_table = DeltaTable.forName(spark, \"my_database.customers\")\n",
    "delta_table.alias(\"target\").merge(\n",
    "    new_data.alias(\"source\"),\n",
    "    \"target.customer_id = source.customer_id\"\n",
    ").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "# Time Travel\n",
    "spark.sql(\"SELECT * FROM my_database.customers VERSION AS OF 1\").show()\n",
    "\n",
    "# Ottimizzazioni\n",
    "spark.sql(\"OPTIMIZE my_database.customers\")\n",
    "spark.sql(\"OPTIMIZE my_database.customers ZORDER BY (customer_id)\")\n",
    "spark.sql(\"VACUUM my_database.customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó 11. Storage Mounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Azure Data Lake (ADLS)\n",
    "configs = {\n",
    "    \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "    \"fs.azure.account.oauth2.client.id\": \"your_client_id\",\n",
    "    \"fs.azure.account.oauth2.client.secret\": \"your_client_secret\",\n",
    "    \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/your_tenant_id/oauth2/token\"\n",
    "}\n",
    "\n",
    "dbutils.fs.mount(\n",
    "    source=\"abfss://container@storageaccount.dfs.core.windows.net/\",\n",
    "    mount_point=\"/mnt/datalake\",\n",
    "    extra_configs=configs\n",
    ")\n",
    "\n",
    "# Mount S3\n",
    "dbutils.fs.mount(\n",
    "    source=\"s3a://your-bucket-name\",\n",
    "    mount_point=\"/mnt/s3bucket\",\n",
    "    extra_configs={\n",
    "        \"fs.s3a.access.key\": \"your_access_key\",\n",
    "        \"fs.s3a.secret.key\": \"your_secret_key\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Gestione mount\n",
    "dbutils.fs.mounts()                    # Lista mount\n",
    "dbutils.fs.unmount(\"/mnt/datalake\")   # Unmount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ 12. Performance Tips (SOLO SPARK!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå MAI usare pandas!\n",
    "# ‚ùå SBAGLIATO: df.toPandas().groupby('col').sum()\n",
    "# ‚úÖ CORRETTO: df.groupBy('col').sum()\n",
    "\n",
    "# Cache per performance\n",
    "df.cache()    # Mantiene in memoria\n",
    "df.persist()  # Pi√π controllo\n",
    "df.unpersist() # Rimuove cache\n",
    "\n",
    "# Broadcast join (tabelle < 200MB)\n",
    "from pyspark.sql.functions import broadcast\n",
    "large_df.join(broadcast(small_df), \"key\").show()\n",
    "\n",
    "# Partizionamento\n",
    "df.repartition(200)  # Ridistribuisce\n",
    "df.coalesce(50)      # Riduce partizioni\n",
    "\n",
    "# Configurazioni\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ 13. ETL Patterns Avanzati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window Functions\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank\n",
    "\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(desc(\"order_date\"))\n",
    "df.withColumn(\"row_num\", row_number().over(window_spec)).show()\n",
    "\n",
    "# Deduplicazione\n",
    "df.dropDuplicates([\"customer_id\", \"email\"]).show()\n",
    "\n",
    "# Slowly Changing Dimension (SCD Type 2)\n",
    "# Pattern complesso per tracking cambiamenti storici\n",
    "def scd_type2_update(target_table, source_df, key_cols):\n",
    "    # Chiudi record vecchi, inserisci nuovi\n",
    "    # Implementazione complessa per produzione\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù 14. Template Esercizio Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPLATE STANDARD PER OGNI ESERCIZIO\n",
    "\n",
    "# 1. Leggi i dati\n",
    "df = spark.read.csv(\"path/to/data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2. Esplora sempre prima!\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "print(f\"Righe totali: {df.count()}\")\n",
    "\n",
    "# 3. Pulisci se necessario\n",
    "df_clean = df.filter(df.column.isNotNull())\n",
    "\n",
    "# 4. Trasforma\n",
    "df_transformed = df_clean.withColumn(\"new_col\", col(\"old_col\") * 2)\n",
    "\n",
    "# 5. Analizza\n",
    "result = df_transformed.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    avg(\"value\").alias(\"avg_value\")\n",
    ")\n",
    "\n",
    "# 6. Mostra risultato\n",
    "result.show()\n",
    "\n",
    "# 7. Salva se richiesto\n",
    "# result.write.saveAsTable(\"my_database.results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
